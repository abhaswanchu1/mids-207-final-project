{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhaswanchu1/mids-207-final-project/blob/main/Final_Experimental_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUMbbCXoAgsQ",
        "outputId": "6bd67eab-d38f-4aed-92fc-048738d0f0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Load in data from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBqjF8hMBy70"
      },
      "source": [
        "# Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZROWhhcWBYa2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import load_npz\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, losses, metrics, optimizers\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style=\"darkgrid\")  # default style\n",
        "import plotly.graph_objs as plotly\n",
        "import pandas as pd\n",
        "\n",
        "path_dir = 'drive/MyDrive/'\n",
        "# Load the .npz files\n",
        "X_train_vectorized = np.load(path_dir+'X_train_vectorized.npy')\n",
        "X_val_vectorized = np.load(path_dir+'X_val_vectorized.npy')\n",
        "X_test_vectorized = np.load(path_dir+'X_test_vectorized.npy')\n",
        "X_train_multihot = np.load(path_dir+'X_train_multihot.npy')\n",
        "X_val_multihot = np.load(path_dir+'X_val_multihot.npy')\n",
        "X_test_multihot = np.load(path_dir+'X_test_multihot.npy')\n",
        "\n",
        "# Load the .npy files\n",
        "y_train = np.load(path_dir+'y_train.npy')\n",
        "y_val = np.load(path_dir+'y_val.npy')\n",
        "y_test = np.load(path_dir+'y_test.npy')\n",
        "y_train_binary = np.load(path_dir+'y_train_binary.npy')\n",
        "y_val_binary = np.load(path_dir+'y_val_binary.npy')\n",
        "y_test_binary = np.load(path_dir+'y_test_binary.npy')\n",
        "y_train_multi = np.load(path_dir+'y_train_multi.npy')\n",
        "y_val_multi = np.load(path_dir+'y_val_multi.npy')\n",
        "y_test_multi = np.load(path_dir+'y_test_multi.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JchqPSwXCAQV"
      },
      "source": [
        "# Baseline Binary Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBJDDao0CD-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d8c259d-e0bc-4723-c5f0-b8b13fc33edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline Accuracy\n",
            "Training set: 0.6932105796001012\n",
            "Validation set: 0.6870728929384966\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "training_baseline_accuracy = accuracy_score(y_train_binary, np.ones(y_train_binary.shape))\n",
        "validation_baseline_accuracy = accuracy_score(y_val_binary, np.ones(y_val_binary.shape))\n",
        "print(f\"\"\"Baseline Accuracy\n",
        "Training set: {training_baseline_accuracy}\n",
        "Validation set: {validation_baseline_accuracy}\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1Q7zbKACeC8"
      },
      "source": [
        "# Experimental Logistic Binary Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfL5dwMJFa9y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "1381109a-9cb7-4ebd-f44e-933089402e45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - f1_score: 0.8205 - loss: 4.9898 - val_f1_score: 0.8145 - val_loss: 0.5909\n",
            "Epoch 2/5\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - f1_score: 0.8188 - loss: 0.5376 - val_f1_score: 0.8144 - val_loss: 0.4295\n",
            "Epoch 3/5\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - f1_score: 0.8188 - loss: 0.4481 - val_f1_score: 0.8145 - val_loss: 0.4364\n",
            "Epoch 4/5\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - f1_score: 0.8188 - loss: 0.4160 - val_f1_score: 0.8145 - val_loss: 0.4031\n",
            "Epoch 5/5\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - f1_score: 0.8190 - loss: 0.3868 - val_f1_score: 0.8145 - val_loss: 0.3508\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m64,128\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m223,493\u001b[0m (873.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">223,493</span> (873.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m74,497\u001b[0m (291.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">74,497</span> (291.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m148,996\u001b[0m (582.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">148,996</span> (582.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "#Create a better binary classification model\n",
        "def build_binary_classification_model(input_shape):\n",
        "  #housekeeping\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(42)\n",
        "  np.random.seed(42)\n",
        "  #Build Model\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  #Compile Model\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=[tf.keras.metrics.F1Score()])\n",
        "  return model\n",
        "binary_model = build_binary_classification_model(input_shape=(X_train_vectorized.shape[1],))\n",
        "y_train_binary = y_train_binary.reshape(-1, 1)\n",
        "y_val_binary = y_val_binary.reshape(-1, 1)\n",
        "y_train_binary = y_train_binary.astype('float32')  # Convert to float32\n",
        "y_val_binary = y_val_binary.astype('float32')\n",
        "history = binary_model.fit(X_train_vectorized, y_train_binary, epochs=5, validation_data=(X_val_vectorized, y_val_binary), verbose = 1)\n",
        "binary_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb1OIYvGFglq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fab5a0c-5594-4440-bc33-f8d271c37955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.81      0.72      0.76      3349\n",
            "    Abnormal       0.88      0.92      0.90      7188\n",
            "\n",
            "    accuracy                           0.86     10537\n",
            "   macro avg       0.84      0.82      0.83     10537\n",
            "weighted avg       0.86      0.86      0.86     10537\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_preds_binary = binary_model.predict(X_test_vectorized)\n",
        "y_preds_binary = (y_preds_binary > 0.5).astype(int)\n",
        "print(classification_report(y_test_binary, y_preds_binary, target_names=['Normal', 'Abnormal'], zero_division=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9niaTXwFicp"
      },
      "source": [
        "# Experimental LSTM Model with Embeddings Binary Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZHSBJVSFula",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27abf66b-6d69-4239-899c-b47c6f4af45a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 40ms/step - f1_score: 0.8188 - loss: 0.6220 - val_f1_score: 0.8145 - val_loss: 0.6217\n",
            "Epoch 2/2\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 39ms/step - f1_score: 0.8188 - loss: 0.6177 - val_f1_score: 0.8145 - val_loss: 0.6217\n"
          ]
        }
      ],
      "source": [
        "#Let's build an LSTM model with embeddings next\n",
        "def build_lstm_model_with_embeddings(vocab_size, embedding_dim, optimizer,input_length):\n",
        "   tf.keras.backend.clear_session()\n",
        "   tf.random.set_seed(42)\n",
        "   np.random.seed(42)\n",
        "   model = tf.keras.Sequential([\n",
        "       tf.keras.layers.Embedding(input_dim = vocab_size, output_dim=embedding_dim, input_length=input_length),\n",
        "       tf.keras.layers.LSTM(128, return_sequences=True),\n",
        "       tf.keras.layers.Dropout(0.2),\n",
        "       tf.keras.layers.LSTM(64),\n",
        "       tf.keras.layers.Dropout(0.2),\n",
        "       tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "   ])\n",
        "\n",
        "   model.compile(loss='binary_crossentropy',\n",
        "                 optimizer=optimizer,\n",
        "                 metrics=['f1_score'])\n",
        "   return model\n",
        "\n",
        "vocab_size = 10000\n",
        "embedding_dim = 128\n",
        "optimizer = 'adam'\n",
        "input_length = X_train_vectorized.shape[1]\n",
        "lstm_model_with_embeddings = build_lstm_model_with_embeddings(vocab_size, embedding_dim, optimizer,input_length)\n",
        "history_1 = lstm_model_with_embeddings.fit(X_train_vectorized, y_train_binary, epochs=2, validation_data=(X_val_vectorized, y_val_binary))\n",
        "history_1 = pd.DataFrame(history_1.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ttrizzDF1kp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2942faf1-ed90-4fff-d6fe-aa7d8fabf01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.00      0.00      0.00      3349\n",
            "    Abnormal       0.68      1.00      0.81      7188\n",
            "\n",
            "    accuracy                           0.68     10537\n",
            "   macro avg       0.34      0.50      0.41     10537\n",
            "weighted avg       0.47      0.68      0.55     10537\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_preds_binary = lstm_model_with_embeddings.predict(X_test_vectorized)\n",
        "y_preds_binary = (y_preds_binary > 0.5).astype(int)\n",
        "print(classification_report(y_test_binary, y_preds_binary, target_names=['Normal', 'Abnormal'], zero_division=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1bj_izKF1_h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c03408b3-d623-4a1a-fd9a-628afdf523f6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[<Embedding name=embedding, built=True>,\n",
              " <LSTM name=lstm, built=True>,\n",
              " <Dropout name=dropout, built=True>,\n",
              " <LSTM name=lstm_1, built=True>,\n",
              " <Dropout name=dropout_1, built=True>,\n",
              " <Dense name=dense, built=True>]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(10000, 128)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Display the model layers.\n",
        "display(lstm_model_with_embeddings.layers)\n",
        "\n",
        "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
        "embeddings = lstm_model_with_embeddings.layers[0].get_weights()[0]\n",
        "display(embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6tmpSr9B2-d"
      },
      "source": [
        "# Baseline multi-class Logistic Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0OzdFtjBo_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8eeee1-2845-4d3e-9aef-a18b587ae3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation F1 Score: 0.38264309055779505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Initialize the model\n",
        "baseline_multi_model = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
        "\n",
        "# Fit the model\n",
        "baseline_multi_model.fit(X_train_vectorized, y_train_multi)\n",
        "\n",
        "# Make predictions on validation data\n",
        "y_val_pred = baseline_multi_model.predict(X_val_vectorized)\n",
        "\n",
        "# Calculate the weighted F1 score\n",
        "val_f1 = f1_score(y_val_multi, y_val_pred, average='weighted')\n",
        "print(f\"Validation F1 Score: {val_f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq6jdwiED0WC"
      },
      "source": [
        "# Experimental LSTM multi-class model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkgczTILECQw"
      },
      "outputs": [],
      "source": [
        "#combining train, test and val into datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train_vectorized, y_train_multi))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test_vectorized, y_test_multi))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val_vectorized, y_val_multi))\n",
        "\n",
        "#configuring dataset for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (tf.cast(x, tf.float32), y))\n",
        "train_ds = train_ds.batch(64)\n",
        "val_ds = val_ds.map(lambda x, y: (tf.cast(x, tf.float32), y))\n",
        "val_ds = val_ds.batch(64)\n",
        "test_ds = test_ds.map(lambda x, y: (tf.cast(x, tf.float32), y))\n",
        "test_ds = test_ds.batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ-zsU3OEiS_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d03c9c55-85e9-4b1a-913c-ce6ae41236f5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed class weights: {0: 1.930495327673609, 1: 2.6781901372648704, 2: 0.48746934809765424, 3: 0.46565211626570807, 4: 6.957517059211974, 5: 2.8596761060345606, 6: 0.7060873450240143}\n",
            "Epoch 1/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 40ms/step - loss: 1.6978 - sparse_categorical_accuracy: 0.3662 - val_loss: 0.9992 - val_sparse_categorical_accuracy: 0.5837\n",
            "Epoch 2/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 1.0779 - sparse_categorical_accuracy: 0.5861 - val_loss: 1.0958 - val_sparse_categorical_accuracy: 0.6035\n",
            "Epoch 3/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - loss: 0.7958 - sparse_categorical_accuracy: 0.6648 - val_loss: 0.8960 - val_sparse_categorical_accuracy: 0.6594\n",
            "Epoch 4/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.6450 - sparse_categorical_accuracy: 0.7113 - val_loss: 0.9219 - val_sparse_categorical_accuracy: 0.6630\n",
            "Epoch 5/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.5470 - sparse_categorical_accuracy: 0.7439 - val_loss: 0.8618 - val_sparse_categorical_accuracy: 0.6922\n",
            "Epoch 6/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - loss: 0.4696 - sparse_categorical_accuracy: 0.7686 - val_loss: 0.8649 - val_sparse_categorical_accuracy: 0.7112\n",
            "Epoch 7/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.3989 - sparse_categorical_accuracy: 0.8017 - val_loss: 0.8364 - val_sparse_categorical_accuracy: 0.7271\n",
            "Epoch 8/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - loss: 0.3843 - sparse_categorical_accuracy: 0.8125 - val_loss: 0.8213 - val_sparse_categorical_accuracy: 0.7152\n",
            "Epoch 9/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.3567 - sparse_categorical_accuracy: 0.8141 - val_loss: 0.7985 - val_sparse_categorical_accuracy: 0.7425\n",
            "Epoch 10/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.3013 - sparse_categorical_accuracy: 0.8408 - val_loss: 0.8552 - val_sparse_categorical_accuracy: 0.7349\n",
            "Epoch 11/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.2796 - sparse_categorical_accuracy: 0.8501 - val_loss: 0.8437 - val_sparse_categorical_accuracy: 0.7312\n",
            "Epoch 12/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.2535 - sparse_categorical_accuracy: 0.8632 - val_loss: 0.9006 - val_sparse_categorical_accuracy: 0.7332\n",
            "Epoch 13/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - loss: 0.2356 - sparse_categorical_accuracy: 0.8693 - val_loss: 0.9481 - val_sparse_categorical_accuracy: 0.7311\n",
            "Epoch 14/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.2310 - sparse_categorical_accuracy: 0.8752 - val_loss: 1.0099 - val_sparse_categorical_accuracy: 0.7297\n",
            "Epoch 15/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 39ms/step - loss: 0.2090 - sparse_categorical_accuracy: 0.8859 - val_loss: 1.0640 - val_sparse_categorical_accuracy: 0.7296\n",
            "Epoch 16/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.2202 - sparse_categorical_accuracy: 0.8860 - val_loss: 1.0255 - val_sparse_categorical_accuracy: 0.7326\n",
            "Epoch 17/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.1952 - sparse_categorical_accuracy: 0.8941 - val_loss: 1.0371 - val_sparse_categorical_accuracy: 0.7323\n",
            "Epoch 18/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.1740 - sparse_categorical_accuracy: 0.9026 - val_loss: 1.1202 - val_sparse_categorical_accuracy: 0.7358\n",
            "Epoch 19/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.1633 - sparse_categorical_accuracy: 0.9082 - val_loss: 1.1644 - val_sparse_categorical_accuracy: 0.7256\n",
            "Epoch 20/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.1631 - sparse_categorical_accuracy: 0.9113 - val_loss: 1.1573 - val_sparse_categorical_accuracy: 0.7237\n",
            "Epoch 21/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.1518 - sparse_categorical_accuracy: 0.9164 - val_loss: 1.1645 - val_sparse_categorical_accuracy: 0.7346\n",
            "Epoch 22/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.1580 - sparse_categorical_accuracy: 0.9174 - val_loss: 1.1782 - val_sparse_categorical_accuracy: 0.7284\n",
            "Epoch 23/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.1378 - sparse_categorical_accuracy: 0.9241 - val_loss: 1.2443 - val_sparse_categorical_accuracy: 0.7262\n",
            "Epoch 24/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.1370 - sparse_categorical_accuracy: 0.9259 - val_loss: 1.2028 - val_sparse_categorical_accuracy: 0.7276\n",
            "Epoch 25/25\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 40ms/step - loss: 0.1291 - sparse_categorical_accuracy: 0.9290 - val_loss: 1.2448 - val_sparse_categorical_accuracy: 0.7281\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 1.3135 - sparse_categorical_accuracy: 0.7228\n",
            "Loss:  1.2723791599273682\n",
            "Accuracy:  0.723640501499176\n",
            "\u001b[1m165/165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step\n",
            "Multi-Class LSTM Model F1 Score: 0.7221399658074511\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers, losses, metrics, optimizers\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Hyperparameters\n",
        "max_features = 10000\n",
        "embedding_dim = 128\n",
        "lstm_units = 192\n",
        "dense1_units = 256\n",
        "dense2_units = 192\n",
        "embedding_dropout = 0.5\n",
        "dense1_dropout = 0.2\n",
        "dense2_dropout = 0.3\n",
        "learning_rate = 0.001\n",
        "num_classes = 7\n",
        "\n",
        "# Build model\n",
        "multi_lstm_model = tf.keras.Sequential([\n",
        "    layers.Embedding(max_features, embedding_dim),\n",
        "    layers.Dropout(embedding_dropout),\n",
        "    layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=False)),\n",
        "    layers.Dropout(dense1_dropout),\n",
        "    layers.Dense(dense1_units, activation='relu'),\n",
        "    layers.Dropout(dense2_dropout),\n",
        "    layers.Dense(dense2_units, activation='relu'),\n",
        "    layers.Dropout(dense2_dropout),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "multi_lstm_model.summary()\n",
        "\n",
        "# Compile model\n",
        "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "multi_lstm_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=optimizer,\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()]\n",
        ")\n",
        "\n",
        "# Assign class weights\n",
        "y_train = np.concatenate([y for x, y in train_ds], axis=0)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "\n",
        "# Convert to class weights to dictionary\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "print(\"Computed class weights:\", class_weights)\n",
        "\n",
        "# Train model\n",
        "epochs = 25\n",
        "history = multi_lstm_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    class_weight=class_weights,  # Add class weights\n",
        "    epochs=epochs\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = multi_lstm_model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "# Grab predictions from test\n",
        "y_true = np.concatenate([y for x, y in test_ds], axis=0)\n",
        "y_pred_probs = multi_lstm_model.predict(test_ds)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "print(\"Multi-Class LSTM Model F1 Score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mivNNfupNI5I"
      },
      "source": [
        "# Experimental Multihot Model for Multiclass Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2QK6YgDNMuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111398d8-41b5-4ec0-8f0d-ff28406bced7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.6075 - loss: 1.0787 - val_accuracy: 0.7618 - val_loss: 0.6307\n",
            "Epoch 2/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8220 - loss: 0.4740 - val_accuracy: 0.7506 - val_loss: 0.6674\n",
            "Epoch 3/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8927 - loss: 0.2937 - val_accuracy: 0.7489 - val_loss: 0.7963\n",
            "Epoch 4/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9404 - loss: 0.1777 - val_accuracy: 0.7507 - val_loss: 0.9555\n",
            "Epoch 5/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9644 - loss: 0.1172 - val_accuracy: 0.7428 - val_loss: 1.0666\n",
            "Epoch 6/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9737 - loss: 0.0876 - val_accuracy: 0.7405 - val_loss: 1.2423\n",
            "Epoch 7/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9809 - loss: 0.0675 - val_accuracy: 0.7297 - val_loss: 1.4787\n",
            "Epoch 8/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9822 - loss: 0.0602 - val_accuracy: 0.7442 - val_loss: 1.3894\n",
            "Epoch 9/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9848 - loss: 0.0519 - val_accuracy: 0.7446 - val_loss: 1.4121\n",
            "Epoch 10/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9894 - loss: 0.0355 - val_accuracy: 0.7402 - val_loss: 1.5022\n",
            "Epoch 11/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9907 - loss: 0.0317 - val_accuracy: 0.7399 - val_loss: 1.5645\n",
            "Epoch 12/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9925 - loss: 0.0244 - val_accuracy: 0.7473 - val_loss: 1.6757\n",
            "Epoch 13/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9926 - loss: 0.0267 - val_accuracy: 0.7446 - val_loss: 1.7230\n",
            "Epoch 14/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9923 - loss: 0.0257 - val_accuracy: 0.7452 - val_loss: 1.7507\n",
            "Epoch 15/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9936 - loss: 0.0210 - val_accuracy: 0.7388 - val_loss: 1.8643\n",
            "Epoch 16/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9931 - loss: 0.0217 - val_accuracy: 0.7428 - val_loss: 1.7874\n",
            "Epoch 17/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9929 - loss: 0.0223 - val_accuracy: 0.7413 - val_loss: 1.8349\n",
            "Epoch 18/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9925 - loss: 0.0226 - val_accuracy: 0.7353 - val_loss: 2.0039\n",
            "Epoch 19/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9941 - loss: 0.0197 - val_accuracy: 0.7383 - val_loss: 1.9646\n",
            "Epoch 20/20\n",
            "\u001b[1m494/494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9939 - loss: 0.0201 - val_accuracy: 0.7417 - val_loss: 2.0175\n",
            "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step\n",
            "Weighted F1-Score: 0.7388\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.71      0.79      0.75       726\n",
            "     Class 1       0.84      0.71      0.77       554\n",
            "     Class 2       0.70      0.64      0.67      3098\n",
            "     Class 3       0.87      0.94      0.90      3297\n",
            "     Class 4       0.76      0.61      0.67       228\n",
            "     Class 5       0.60      0.48      0.53       544\n",
            "     Class 6       0.60      0.65      0.63      2089\n",
            "\n",
            "    accuracy                           0.74     10536\n",
            "   macro avg       0.73      0.69      0.70     10536\n",
            "weighted avg       0.74      0.74      0.74     10536\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Model for multi-hot encoded data\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 10000  # Vocabulary size\n",
        "learning_rate = 0.001  # Best learning rate\n",
        "\n",
        "# Define the model for multi-hot encoded data\n",
        "model = Sequential([\n",
        "    # Input layer for multi-hot encoded data\n",
        "    layers.Dense(256, activation='relu', input_shape=(vocab_size,)),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Single Dense Layer\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Output layer for multi-class classification (7 classes)\n",
        "    layers.Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_multihot, y_train_multi,  # Use multi-hot encoded data\n",
        "    validation_data=(X_val_multihot, y_val_multi),  # Use multi-hot encoded validation data\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Predict on validation data\n",
        "y_val_pred = model.predict(X_val_multihot)\n",
        "y_val_pred_classes = tf.argmax(y_val_pred, axis=1).numpy()  # Convert probabilities to class predictions\n",
        "\n",
        "# Calculate weighted F1 score\n",
        "weighted_f1 = f1_score(y_val_multi, y_val_pred_classes, average='weighted')\n",
        "print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val_multi, y_val_pred_classes, target_names=[f\"Class {i}\" for i in range(7)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHjqGqkiNM_S"
      },
      "source": [
        "# Transformer for Multiclass Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChA_pNyRNQJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b450b986-fb03-4255-fe7f-e4f0483b5663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 20ms/step - accuracy: 0.4770 - loss: 1.4283 - val_accuracy: 0.5196 - val_loss: 1.3127\n",
            "Epoch 2/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5137 - loss: 1.3425 - val_accuracy: 0.5186 - val_loss: 1.3130\n",
            "Epoch 3/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5142 - loss: 1.3347 - val_accuracy: 0.5185 - val_loss: 1.3150\n",
            "Epoch 4/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5170 - loss: 1.3302 - val_accuracy: 0.5196 - val_loss: 1.3126\n",
            "Epoch 5/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5178 - loss: 1.3269 - val_accuracy: 0.5190 - val_loss: 1.3123\n",
            "Epoch 6/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5166 - loss: 1.3252 - val_accuracy: 0.5201 - val_loss: 1.3084\n",
            "Epoch 7/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5180 - loss: 1.3215 - val_accuracy: 0.5206 - val_loss: 1.3084\n",
            "Epoch 8/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5171 - loss: 1.3217 - val_accuracy: 0.5208 - val_loss: 1.3075\n",
            "Epoch 9/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5176 - loss: 1.3219 - val_accuracy: 0.5211 - val_loss: 1.3059\n",
            "Epoch 10/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5177 - loss: 1.3193 - val_accuracy: 0.5216 - val_loss: 1.3055\n",
            "Epoch 11/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5186 - loss: 1.3192 - val_accuracy: 0.5207 - val_loss: 1.3061\n",
            "Epoch 12/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5194 - loss: 1.3175 - val_accuracy: 0.5205 - val_loss: 1.3027\n",
            "Epoch 13/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5183 - loss: 1.3198 - val_accuracy: 0.5212 - val_loss: 1.3036\n",
            "Epoch 14/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5182 - loss: 1.3166 - val_accuracy: 0.5203 - val_loss: 1.3045\n",
            "Epoch 15/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5191 - loss: 1.3169 - val_accuracy: 0.5210 - val_loss: 1.3061\n",
            "Epoch 16/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5190 - loss: 1.3152 - val_accuracy: 0.5216 - val_loss: 1.3020\n",
            "Epoch 17/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5183 - loss: 1.3147 - val_accuracy: 0.5211 - val_loss: 1.3017\n",
            "Epoch 18/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5199 - loss: 1.3130 - val_accuracy: 0.5205 - val_loss: 1.3025\n",
            "Epoch 19/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5193 - loss: 1.3121 - val_accuracy: 0.5210 - val_loss: 1.3017\n",
            "Epoch 20/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.5200 - loss: 1.3130 - val_accuracy: 0.5222 - val_loss: 1.3009\n",
            "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5275 - loss: 1.2890\n",
            "Test Accuracy: 0.5253867506980896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (32, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step\n",
            "Weighted F1-Score: 0.4103\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.00      0.00      0.00       776\n",
            "     Class 1       0.00      0.00      0.00       537\n",
            "     Class 2       0.40      0.87      0.55      3043\n",
            "     Class 3       0.73      0.87      0.79      3349\n",
            "     Class 4       0.00      0.00      0.00       200\n",
            "     Class 5       0.00      0.00      0.00       464\n",
            "     Class 6       0.00      0.00      0.00      2168\n",
            "\n",
            "    accuracy                           0.53     10537\n",
            "   macro avg       0.16      0.25      0.19     10537\n",
            "weighted avg       0.35      0.53      0.41     10537\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "#Try a transformer model\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = models.Sequential([\n",
        "            layers.Dense(ff_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # Multi-head attention\n",
        "        attention_output = self.attention(inputs, inputs)\n",
        "        attention_output = self.dropout1(attention_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "\n",
        "        # Feedforward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_transformer_model(input_shape, num_classes, embed_dim=64, num_heads=4, ff_dim=128, num_layers=2, dropout_rate=0.1):\n",
        "    inputs = layers.Input(shape=input_shape)  # Input shape: (500,)\n",
        "\n",
        "    # Project to embedding dimension and reshape to (batch_size, sequence_length, embed_dim)\n",
        "    x = layers.Dense(embed_dim)(inputs)  # Add embedding dimension\n",
        "    x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(x)  # Expand to 3D for MultiHeadAttention\n",
        "\n",
        "    # Add Transformer Encoder layers\n",
        "    for _ in range(num_layers):\n",
        "        x = TransformerEncoder(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
        "\n",
        "    # Global average pooling and output\n",
        "    x = layers.GlobalAveragePooling1D()(x)  # Pool across the sequence\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return models.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# Set input shape and number of classes\n",
        "input_shape = (X_train_vectorized.shape[1],)  # Shape of vectorized input (500,)\n",
        "num_classes = len(np.unique(y_train_multi))  # Number of classes\n",
        "\n",
        "# Build the model\n",
        "transformer_model = build_transformer_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "transformer_model.compile(optimizer='adam',\n",
        "                          loss='sparse_categorical_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "transformer_model.fit(\n",
        "    X_train_vectorized, y_train_multi,\n",
        "    validation_data=(X_val_vectorized, y_val_multi),\n",
        "    epochs=20,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = transformer_model.evaluate(X_test_vectorized, y_test_multi)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predict on validation data\n",
        "y_test_pred = transformer_model.predict(X_test_vectorized)\n",
        "y_test_pred_classes = tf.argmax(y_test_pred, axis=1).numpy()  # Convert probabilities to class predictions\n",
        "\n",
        "# Calculate weighted F1 score\n",
        "weighted_f1 = f1_score(y_test_multi, y_test_pred_classes, average='weighted')\n",
        "print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_multi, y_test_pred_classes, target_names=[f\"Class {i}\" for i in range(7)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IagLQCF_PIsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4c3835-1106-4d0e-ed75-6e947fa4c57e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [00h 00m 50s]\n",
            "val_accuracy: 0.5211655497550964\n",
            "\n",
            "Best val_accuracy So Far: 0.5245823860168457\n",
            "Total elapsed time: 00h 23m 54s\n",
            "\n",
            "The optimal hyperparameters:\n",
            "- embed_dim = 128\n",
            "- num_heads = 8\n",
            "- ff_dim = 64\n",
            "- num_layers = 4\n",
            "- dropout_rate = 0.5\n",
            "- learning_rate = 0.0012192341843725888\n",
            "\n",
            "\n",
            "Epoch 1/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 20ms/step - accuracy: 0.4164 - loss: 1.7080 - val_accuracy: 0.5169 - val_loss: 1.3718\n",
            "Epoch 2/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5068 - loss: 1.4329 - val_accuracy: 0.5207 - val_loss: 1.3588\n",
            "Epoch 3/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5120 - loss: 1.3994 - val_accuracy: 0.5207 - val_loss: 1.3495\n",
            "Epoch 4/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5066 - loss: 1.4029 - val_accuracy: 0.5210 - val_loss: 1.3556\n",
            "Epoch 5/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.5132 - loss: 1.3879 - val_accuracy: 0.5220 - val_loss: 1.3552\n",
            "Epoch 6/20\n",
            "\u001b[1m988/988\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.5130 - loss: 1.3858 - val_accuracy: 0.5193 - val_loss: 1.3582\n",
            "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.5212 - loss: 1.3383\n",
            "Test Accuracy: 0.5183638334274292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (32, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step\n",
            "Weighted F1-Score: 0.4009\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.00      0.00      0.00       776\n",
            "     Class 1       0.00      0.00      0.00       537\n",
            "     Class 2       0.40      0.82      0.54      3043\n",
            "     Class 3       0.68      0.89      0.77      3349\n",
            "     Class 4       0.00      0.00      0.00       200\n",
            "     Class 5       0.00      0.00      0.00       464\n",
            "     Class 6       0.00      0.00      0.00      2168\n",
            "\n",
            "    accuracy                           0.52     10537\n",
            "   macro avg       0.15      0.24      0.19     10537\n",
            "weighted avg       0.33      0.52      0.40     10537\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "#Use Keras tuner to try different hyperparameters for transformer model\n",
        "!pip install keras-tuner\n",
        "import keras_tuner\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def build_transformer_model(hp):\n",
        "    input_shape = (X_train_vectorized.shape[1],)  # Shape of vectorized input (500,)\n",
        "    num_classes = len(np.unique(y_train))  # Number of classes\n",
        "\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Hyperparameters to tune (optimized search space)\n",
        "    embed_dim = hp.Int(\"embed_dim\", min_value=32, max_value=128, step=32)\n",
        "    num_heads = hp.Choice(\"num_heads\", values=[2, 4, 8])\n",
        "    ff_dim = hp.Int(\"ff_dim\", min_value=64, max_value=256, step=64)\n",
        "    num_layers = hp.Int(\"num_layers\", min_value=1, max_value=4, step=1)\n",
        "    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.1, max_value=0.5, step=0.1)\n",
        "\n",
        "    # Project to embedding dimension and reshape to (batch_size, sequence_length, embed_dim)\n",
        "    x = layers.Dense(embed_dim)(inputs)\n",
        "    x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(x)  # Expand to 3D for MultiHeadAttention\n",
        "\n",
        "    # Add Transformer Encoder layers\n",
        "    for _ in range(num_layers):\n",
        "        x = TransformerEncoder(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
        "\n",
        "    # Global average pooling and output\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "\n",
        "    # Additional hyperparameter: learning rate\n",
        "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=5e-3, sampling=\"log\")\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "from keras_tuner import Hyperband\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = Hyperband(\n",
        "    build_transformer_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_epochs=20,\n",
        "    factor=3,\n",
        "    directory=\"transformer_tuning\",\n",
        "    project_name=\"transformer_full_dataset_tuning\"\n",
        ")\n",
        "\n",
        "# Display the search space summary\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Define early stopping to avoid overfitting\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Perform the search on the full dataset\n",
        "tuner.search(\n",
        "    X_train_vectorized, y_train_multi,\n",
        "    validation_data=(X_val_vectorized, y_val_multi),\n",
        "    epochs=30,  # Max epochs for each trial\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The optimal hyperparameters:\n",
        "- embed_dim = {best_hps.get('embed_dim')}\n",
        "- num_heads = {best_hps.get('num_heads')}\n",
        "- ff_dim = {best_hps.get('ff_dim')}\n",
        "- num_layers = {best_hps.get('num_layers')}\n",
        "- dropout_rate = {best_hps.get('dropout_rate')}\n",
        "- learning_rate = {best_hps.get('learning_rate')}\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "# Train the best model on the full dataset\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "history = best_model.fit(\n",
        "    X_train_vectorized, y_train_multi,\n",
        "    validation_data=(X_val_vectorized, y_val_multi),\n",
        "    epochs=20,  # Train the final model for more epochs\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the best model\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test_vectorized, y_test_multi)\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predict on test data\n",
        "y_test_pred = best_model.predict(X_test_vectorized)\n",
        "y_test_pred_classes = tf.argmax(y_test_pred, axis=1).numpy()  # Convert probabilities to class predictions\n",
        "\n",
        "# Calculate weighted F1 score\n",
        "weighted_f1 = f1_score(y_test_multi, y_test_pred_classes, average='weighted')\n",
        "print(f\"Weighted F1-Score: {weighted_f1:.4f}\")\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_multi, y_test_pred_classes, target_names=[f\"Class {i}\" for i in range(7)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words Model\n"
      ],
      "metadata": {
        "id": "MerB4GnQcLMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Trying a bag of words model\n",
        "# Load the sparse datasets from Google Drive\n",
        "from scipy.sparse import load_npz\n",
        "import numpy as np\n",
        "\n",
        "# Load sparse matrices\n",
        "X_train_bow = load_npz('/content/drive/MyDrive/X_train_bow.npz')\n",
        "X_val_bow = load_npz('/content/drive/MyDrive/X_val_bow.npz')\n",
        "X_test_bow = load_npz('/content/drive/MyDrive/X_test_bow.npz')\n",
        "\n",
        "\n",
        "\n",
        "print(\"Datasets loaded successfully from Google Drive.\")"
      ],
      "metadata": {
        "id": "ViDTu_wDcVpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f85ad751-373d-4320-a675-471a1c43bbbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets loaded successfully from Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "\n",
        "#Train the model\n",
        "model = LogisticRegression(max_iter=1000)  # Increase max_iter for convergence on large datasets\n",
        "model.fit(X_train_bow, y_train_multi)\n",
        "\n",
        "#Validate the model\n",
        "y_val_pred = model.predict(X_val_bow)\n",
        "val_f1 = f1_score(y_val_multi, y_val_pred, average='weighted')\n",
        "\n",
        "print(\"Validation Weighted F1 Score:\", val_f1)\n",
        "print(\"Validation Classification Report:\\n\", classification_report(y_val_multi, y_val_pred))\n",
        "\n",
        "#Test the model\n",
        "y_test_pred = model.predict(X_test_bow)\n",
        "test_f1 = f1_score(y_test_multi, y_test_pred, average='weighted')\n",
        "\n",
        "print(\"Test Weighted F1 Score:\", test_f1)\n",
        "print(\"Test Classification Report:\\n\", classification_report(y_test_multi, y_test_pred))"
      ],
      "metadata": {
        "id": "-FH4P_MxcgHd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333ded90-665c-4dc8-da3c-b5e70057dc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Weighted F1 Score: 0.7416785669535761\n",
            "Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.73      0.74       726\n",
            "           1       0.83      0.71      0.77       554\n",
            "           2       0.70      0.68      0.69      3098\n",
            "           3       0.87      0.95      0.91      3297\n",
            "           4       0.73      0.56      0.63       228\n",
            "           5       0.56      0.49      0.52       544\n",
            "           6       0.62      0.62      0.62      2089\n",
            "\n",
            "    accuracy                           0.75     10536\n",
            "   macro avg       0.72      0.68      0.70     10536\n",
            "weighted avg       0.74      0.75      0.74     10536\n",
            "\n",
            "Test Weighted F1 Score: 0.738521254706624\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.74      0.76       776\n",
            "           1       0.83      0.67      0.74       537\n",
            "           2       0.68      0.68      0.68      3043\n",
            "           3       0.86      0.95      0.90      3349\n",
            "           4       0.69      0.52      0.59       200\n",
            "           5       0.56      0.53      0.54       464\n",
            "           6       0.62      0.60      0.61      2168\n",
            "\n",
            "    accuracy                           0.74     10537\n",
            "   macro avg       0.72      0.67      0.69     10537\n",
            "weighted avg       0.74      0.74      0.74     10537\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}